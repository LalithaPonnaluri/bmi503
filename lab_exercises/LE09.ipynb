{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e89443",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "Random forest classifers are similar to decision trees in that they use hierarchical structures to split the dataset based on features. However, unlike decision trees, these classifiers use muliple decision trees (a \"forest\") in classification process using a method called *bagging*. Random forest is called an *ensemble* method because we have multiple classifiers by which we make our final prediction.\n",
    "\n",
    "The random forest algorithm consists of four general steps:\n",
    "* Select random samples from a given dataset - *bootstrapping*.\n",
    "* Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
    "* Perform a vote for each predicted result.\n",
    "* Select the prediction result with the most votes as the final prediction - *aggregating*.\n",
    "\n",
    "<img width=\"500px\" src=\"img/random_forest_voting.png\" />\n",
    "\n",
    "**Advantages**\n",
    "* Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "* It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
    "* The algorithm can be used in both classification and regression problems.\n",
    "* Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
    "* You can get the relative feature importance, which helps in selecting the most contributing features for the classifier.\n",
    "\n",
    "**Disadvantages**\n",
    "* Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
    "* The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9f610",
   "metadata": {},
   "source": [
    "## Implementing random forest\n",
    "Like decision trees, building and fitting a random forest classifier is a straightforward task  in scikit-learn. First, we define a random forest classifier variable, and, second, we train the classifier by calling the `fit` method.\n",
    "\n",
    "Random forest has many hyperparameters. Hyperparameters included in Random Forest are:\n",
    "* `n_estimators` = number of trees in the forest\n",
    "* `criterion` = the criterion used to choose a split at each node (e.g. gini, entropy, mse, etc.)\n",
    "* `max_depth` = maximum length of the longest route in each tree\n",
    "* `min_samples_split` = minimum number of samples to split on at a node\n",
    "* `max_leaf_nodes` = maximum number of leaf nodes\n",
    "* `max_features` = maximum number of random features to test at each node\n",
    "* `max_samples` = size of bootstrapped dataset for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b65f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz # Import Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "        roc_auc_score, auc, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "from IPython.display import Image \n",
    "\n",
    "import random\n",
    "## set seed for randomization\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build and fit random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208230b",
   "metadata": {},
   "source": [
    "## Evaluating random forest\n",
    "We can evaluate the our random forest classifier by calculating the accuracy, recall, precision, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = rfc.predict(X_test)\n",
    "y_proba_forest = list(zip(*rfc.predict_proba(X_test)))[1]\n",
    "accuracy_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b6972",
   "metadata": {},
   "source": [
    "As before, we can display the `confusion_matrix` of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get values for confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_forest).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccb83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## show confusion matrix for random forest\n",
    "show_confusion_matrix(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_static_roc_curve(fpr, tpr):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(fpr, tpr, alpha=.5, color='darkorange')\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "    plt.plot([0,1], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC curve\");\n",
    "    \n",
    "def plot_static_pr_curve(recall, precision):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(recall, precision, alpha=.5, color='darkorange')\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2)\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot([1,0], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-recall curve\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_proba_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_forest)\n",
    "plot_static_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbcabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_forest)\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d228eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_static_pr_curve(recall,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2deb83c",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Cross-validation is key to choosing the best possible hyperparameters. This involves splitting the training set into $k$ number of subsets where one subset is used as a validation set and the remaining $k-1$ are used for training. This is then completed over all possible sets of $k$ and the average of the metrics is used to assess the model with the given hyperparameters.\n",
    "\n",
    "To further this idea, we can use cross-validation in concert with a *grid search* which runs a model with variable hyperparameters that are defined by lists of values. This will \"check\" the metrics for each of this runs and average them. The optimal combination of hyperparameters will be outputted as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5794d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees to be used\n",
    "rfc_n_estimators = [int(x) for x in np.linspace(100, 500, 5)]\n",
    "# Maximum length in tree\n",
    "rfc_max_depth = [int(x) for x in np.linspace(2, 10, 5)]\n",
    "\n",
    "rfc_grid = {'n_estimators': rfc_n_estimators,\n",
    "            'max_depth': rfc_max_depth}\n",
    "\n",
    "# Create the model to be tuned\n",
    "rfc_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the random search Random Forest\n",
    "rfc_random = RandomizedSearchCV(estimator = rfc_base, param_distributions = rfc_grid, \n",
    "                                n_iter = 200, cv = 4, scoring='f1',\n",
    "                                random_state = 42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rfc_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal parameters\n",
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = rfc_random.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bfcb6",
   "metadata": {},
   "source": [
    "## Feature ranking\n",
    "In addition to evaluating the random forest classifier, it is sometimes helpful to see how important each of the features were in arriving at final predictions. If we notice that a feature is of little importance, we can eliminate it from our training dataset in order to gain efficiency.\n",
    "\n",
    "When building a random forest classifier, scikit-learn returns a variable named `feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b639337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find important features\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd733a6",
   "metadata": {},
   "source": [
    "The raw output is a little difficult to interpret. So, we will put the output in a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a664424",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = \\\n",
    "    pds.Series(rfc.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb0848",
   "metadata": {},
   "source": [
    "We can also visualize the feature importances using a seaborn barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21775c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize important features\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('\\nFeature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68785fab",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "How does this differ from Random Forest? Random Forest uses bagging in order to train a final model. XGBoost works by a method called **boosting**, which is an iterative, sequential method that adds a new decision tree to the overall model at each step to minimize error from the previous trees. Each new tree is a *weak learner* that when all combined creates a strong learner that will accurately predict the outcome.\n",
    "\n",
    "<img width=\"500px\" src=\"img/xgboost_boosting.png\" />\n",
    "\n",
    "A problem with XGBoost is that it is highly sensitive to it's hyperparameters. If too many trees are added, it can be overfit. Moreover, the `learning rate` is crucial because the model will perform better if trained slowly, but the likelihood of many trees being created increases with a decreaed learning rate. FInding the right balance for the model is key to the robustness and generalizability of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a827576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d719a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## build and fit XGBoost classifier\n",
    "xgc = xgb.XGBClassifier(objective='reg:logistic',n_estimators=100, \\\n",
    "                        alpha=0.01, max_depth=4, learning_rate=0.1, \\\n",
    "                        colsample_bytree=0.3, use_label_encoder=False)\n",
    "xgc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_boost = xgc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbd94c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cba25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = \\\n",
    "    pds.Series(xgc.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "## visualize important features\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('\\nFeature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31580fb-e6db-4dec-a92a-da2bf3fb79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score, auc, precision_recall_curve, roc_curve,\\\n",
    "    classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f0a98-6e08-401f-9d34-9c6cd55e2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Pima Indians Diabetes dataset (downloaded May 14, 2019; N=768)\n",
    "df = pds.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cdf06-698a-4bce-9b95-73d9a609c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_test, y_pred, palette=\"inferno\"):\n",
    "    ## see: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "    ##      https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "    ##      https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    colors = sns.color_palette(palette) # set the colors to use for heatmap\n",
    "    # print(colors.as_hex()) # uncomment this to see color palette\n",
    "\n",
    "    ax = sns.heatmap(matrix, square=True, annot=True, fmt='d', \n",
    "                     cbar=False, cmap=colors, vmin=-1, annot_kws={\"size\":13}, linewidths=1.0)\n",
    "\n",
    "    # set labels on figure\n",
    "    ax.set_xticklabels(labels=[\"neg\",\"pos\"], fontsize=13)\n",
    "    ax.set_yticklabels(labels=[\"neg\",\"pos\"], fontsize= 13)\n",
    "    plt.xlabel(\"\\nactual value\", fontsize=15)\n",
    "    plt.ylabel(\"predicted value\\n\", fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_static_roc_curve(fpr, tpr):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(fpr, tpr, alpha=.5, color='darkorange')\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "    plt.plot([0,1], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC curve\");\n",
    "    \n",
    "def plot_static_pr_curve(recall, precision):\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.fill_between(recall, precision, alpha=.5, color='darkorange')\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2)\n",
    "    # Add dashed line with a slope of 1\n",
    "    plt.plot([1,0], [0,1], linestyle=(0, (5, 5)), linewidth=2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-recall curve\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0ab84-6253-4000-a4ed-bbf15dc2867e",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Now that we understand and have run a logistic regression model, let's go a bit \"deeper\". We can think of a neural network (NN) as a set of nested functions -- we call these layers. Each layer in our model takes input from the previous layer and outputs directly to the next layer, i.e. fully connected. \n",
    "\n",
    "We are going to create a 3 layer neural network with the previously used 8 variables as features and the \"Outcome\" as the label. \n",
    "\n",
    "The first layer of our NN will take in all 8 features as input, has a ReLU (rectified linear unit) activation function, and outputs 12 latent features (hidden). As opposed to the logistic function, discussed previously, ReLU sets the input to 0 if it is <0 or uses the input as is if >0.\n",
    "\n",
    "$f(x)=max(0,x)$\n",
    "\n",
    "The second layer of our NN will take in all 12 latent features from the previous layer as input, has a ReLU (rectified linear unit) activation function, and outputs 8 latent features.\n",
    "\n",
    "The third (and last) layer of our model is a sigmoid output layer that takes in the previous 8 latent features as input.\n",
    "\n",
    "The loss function we use for this model is binary cross entropy, which basically sums the log probabilty of a given sample being in the 0 class and the log probability of the sample being in the 1 class across all samples. This is essentially the same function as the log likelihood. We want to minimize this loss function.\n",
    "\n",
    "$ \\ln Loss = \\sum_{i=1}^{N}-(y_{i}\\ln f(x_{i})+(1-{y_{i}}) \\ln (1-f(x_{i}))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1a254-e3be-4759-8431-ddbecb1e77d5",
   "metadata": {},
   "source": [
    "For our implementation of neural network, we will use keras's sequential model:\n",
    "* https://keras.io/guides/sequential_model/\n",
    "\n",
    "Let's load the libraries we will be using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bb606-8e37-4feb-8819-cfff55e8f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccefa4-f940-4876-8195-d63e0cb702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=75, batch_size=10)\n",
    "\n",
    "# make class predictions with the model\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = (y_proba > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50deb0a9-5333-412d-af40-032b14fa2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show confustion matrix\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c3378-cd42-4d3e-960a-b299b7e8bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcc04b-b277-4780-819f-982c7ec893cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate F1 score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063c9b4-99ad-4494-b6da-1d1880ea4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUROC\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0b3f9-8613-4639-862b-fa3b4f37ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "plot_static_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b62df5-2aa3-4b66-a7ad-d64ea52deca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate PR curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "# calculate AUPR\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479dabf6-3f06-40fb-aa17-31cfe9cfa4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_static_pr_curve(recall,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede0402",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "---\n",
    "\n",
    "Without labels, we can still use machine learning to extract informstion from data, such as how to group the data, what patterns exist in the data, or how to restructure the data to be more concise without losing information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88639f",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "This is one of the most common methods when discussing clustering. It works by the following steps:\n",
    "\n",
    "1. Guess some cluster centers\n",
    "2. E-Step: assign data points to the nearest cluster center\n",
    "3. M-Step: set the cluster centers to the mean of each cluster\n",
    "4. Repeat steps 3 and 4 until converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=42)\n",
    "df = pd.DataFrame(X)\n",
    "sns.scatterplot(data=df, x=0, y=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2b735-89bd-494b-9213-a7a01d3781cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f26c8-d26a-4ead-bbce-ec919ddeac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "df['clusters'] = y_kmeans\n",
    "sns.scatterplot(data=df, x=0, y=1, hue='clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629521a1-a572-42aa-888d-90c11191ae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
