{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a50f33a",
   "metadata": {},
   "source": [
    "# Lab Exercise 07\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d02a49",
   "metadata": {},
   "source": [
    "## Analysis techniques\n",
    "\n",
    "\n",
    "### Correlation analyses\n",
    "\n",
    "We are going to look at a few correlation analysis techniques that can be used in Python. These functions are implemented in numerous packages, so we will check out a few to get a taste for different python modules.\n",
    "\n",
    "As for the correlation methods, there are 4 we will focus on here:\n",
    "1. Pearson correlation \n",
    "2. Spearman correlation\n",
    "3. Kendall rank correlation (or Kendalls' tau)\n",
    "4. Point biserial correlation\n",
    "\n",
    "Each of these tests is used in different scenarios; however, this is not a stats class, so we will not be diving too deeply into the theory that is covered in a more formal stats class. I will briefly describe each and explain when it is appropriate to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbf171",
   "metadata": {},
   "source": [
    "**Pearson correlation** is used to calculate linear relationship between two continuous variables. The relationship is defined by the \"correlation coefficient\" that is calculated, along with the p-value. The correlation coefficient can fall within the range -1 to +1, with -1 being a perfect negative relationship (left), +1 being a perfect positive relationship (right), and 0 being weak or no relationship (middle)\n",
    "\n",
    "![corr](img/pearson_corr.png)\n",
    "\n",
    "To use this method, you must ensure that you satisfy all assumptions. These include:\n",
    "1. Level of measurement: must be interval or ratio (continuous variable)\n",
    "2. Normality: Both variables must have roughly normal distributions\n",
    "3. Linear relationship: The two variables have a linear relationship as opposed to a non-linear relationship.\n",
    "4. Paired data: For each measurement for one variable and paired value in the other variable must exist.\n",
    "5. No outliers: check for extreme outliers present in the two variables.\n",
    "\n",
    "We know some of these assumptions are true by understanding our data, i.e., each patient has a bmi and age, so they are paired data. We can check the other assumptions pretty quickly visually using some plotting techniques.\n",
    "\n",
    "First, let's load up our dataset in to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a6b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"stroke_data.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373ca7c",
   "metadata": {},
   "source": [
    "Let's check if the two variables (age and bmi) have \"roughly\" a linear relationship. We can check this visually using `relplot` in seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4385a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.relplot(df, x='age', y='bmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b80b4",
   "metadata": {},
   "source": [
    "You can tell, it is not exactly linear, but we will proceed regardless and chec kthe other assumptions.\n",
    "\n",
    "Next, let us check if each variable is nromally distributed. We will do this by using the `displot` function in seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2944723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x='age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee080ed",
   "metadata": {},
   "source": [
    "This doesn't look bad...somewhat normal. Using the kernal density estimation (kde) may be more helpful in visualizing whether the plot is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9d29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x='age', kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2643403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04eb15d",
   "metadata": {},
   "source": [
    "We see a bit better from the kde that the distribution might have some heavy tails.\n",
    "\n",
    "We can use another method to determine normality. This is called the Q-Q plot or quantile-quantile plot, which plots the calculated quantiles for a given variable against the theoretical quantiles. If normal, the plotted points should fall along the diagonal line.\n",
    "\n",
    "We will leverage the function built in `statsmodel` called `qqplot` as shown below. The line to fit against will be `\"s\"` which means the standardized line with respect to the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ca227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(df.age, line='s', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842132f",
   "metadata": {},
   "source": [
    "Now we are pretty confident that the age variable is not normally distributed. There are ways to account for this, e.g., taking the log of the values, etc. Here we will proceed for the sake of learning the other assumptions and running the test.\n",
    "\n",
    "Let's check the bmi variable quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36891e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x='bmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbcfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x='bmi', kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a851ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.qqplot(df.bmi, line='s', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f35cde",
   "metadata": {},
   "source": [
    "The last assumption we will check is the presence of extreme outliers in the data. A quick visual check is using a boxplot to see if any values fall outside the \"min\" or \"max\" of the whiskers.\n",
    "\n",
    "We will use the `catplot` function in seaborn to plot these boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccddc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(df, x='bmi', kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27169ac1",
   "metadata": {},
   "source": [
    "As you can clearly see there are quite a few outliers in the bmi variable, which we partially saw with the tail in the histogram plot.\n",
    "\n",
    "If we are concerned about these outliers, we can always remove them with a fairly simple line of code using pandas and `numpy` and the `stats` subpackage within `scipy`. We will calculate the absolute value of the z score for each of the values in the bmi variable and only keep those that are less than 3 (within the 99.7% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32317921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = df[(np.abs(stats.zscore(df['bmi'])) < 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa754adc",
   "metadata": {},
   "source": [
    "What does the boxplot look like now without those outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfea56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(df, x='bmi', kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ca164",
   "metadata": {},
   "source": [
    "Fewer outliers, especially those more extreme ones. This can be a way of handling this assumption violation, if it makes sense to do so.\n",
    "\n",
    "Let's check if there are any outliers for the age variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b35f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(df, x='age', kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6eea14",
   "metadata": {},
   "source": [
    "Nope. Looks like we are good with the age variable and the \"no outliers\" assumption.\n",
    "\n",
    "Even though we have violated at least one of the assumptions above, we will proceed with running the Pearson correlation test on these two variables.\n",
    "\n",
    "To do so, we will again use the `stats` subpackage from `scipy`, specifically the `pearsonr` function. This function takes in the two set of data (age and bmi variables) and outputs a list containing two values: the correlation (as described above) and the p-value to determine the significance of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea07204",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, p = stats.pearsonr(df['age'], df['bmi'])\n",
    "print(f\"Pearson correlation = {corr} and p-value = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2d285",
   "metadata": {},
   "source": [
    "If we ignore the fact that we violated the assumptions, the above results would tell us that the associaiton between the two variables is signfiicant (p<<<0.05), but it is a weak association (0.37)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f5fb4",
   "metadata": {},
   "source": [
    "Returning to the idea that we vioalted the assumptions for the Pearson correlation, what other methods can be used to determine correlation for data that fails any of the assumptions?\n",
    "\n",
    "**Spearman rank correlation** is a non-parametric method used to calculate monotonic relationship between two variables. Also falls between -1 and +1, but is not directly concerned with the lienar nature of the relationship between two variables. This means Spearman correlation is less restrictive than pearson.\n",
    "\n",
    "We can use this test to determine if the age varaible has a monotonic relationship with bmi.\n",
    "\n",
    "This function can also be found in `scipy` and is calle `spearmanr`. The output is simialr to Pearson as it generated a list that contains the correlation value (rho) and the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68302567",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, p = stats.spearmanr(df['age'], df['bmi'])\n",
    "print(f\"Spearman correlation (rho) = {corr} and p-value = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563189b1",
   "metadata": {},
   "source": [
    "As we saw with the Pearson correlation results, there is a weak relationship between these two variables.\n",
    "\n",
    "**Kendall's tau** is yet another non-parametric correlation method that can be used when your data violates a parametric method like Pearson. This is very similar to Spearman in how the results are interpreted, measures the strength of relationship between two variables.\n",
    "\n",
    "The `scipy` function `kendalltau` will calcualte the tau statistic (correlation coefficient) and the p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6564f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr, p = stats.kendalltau(df['age'], df['bmi'])\n",
    "print(f\"Kendall's tau = {corr} and p-value = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60757b",
   "metadata": {},
   "source": [
    "What if we are interested in assessing the correlation between a categorical and continuous variable? To do this we \n",
    "**Point biserial correlation**. Like Pearson, Point bierial corelation has a few assumptions that must be met. These include: \n",
    "1. Continuous and Binary\n",
    "2. Normally Distributed\n",
    "3. No Outliers\n",
    "4. Equal Variances\n",
    "\n",
    "We have already seen how to check the first three, to check the fourth we can visually inspect by plotting two histograms colored by the binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcf795",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df,x=df['bmi'],hue=df['heart_disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a7939",
   "metadata": {},
   "source": [
    "However, if we want to be more quantitative in our assessment we can calculater the standard deviation of the bmi variable with respect to the heart_disease variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9620176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[(df['heart_disease']==0),'bmi'].std())\n",
    "print(df.loc[(df['heart_disease']==1),'bmi'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65267a4c",
   "metadata": {},
   "source": [
    "If we do not violate any of the assumptions, we can use the point biserial correlation to assess the strength of relationship between the continuous and categorical variable. Here we use the `scipy` package again with the `pointbiserialr` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0eeed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr, p = stats.pointbiserialr(df['age'], df['stroke'])\n",
    "print(f\"Point biserial correlation = {corr} and p-value = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d92fdf",
   "metadata": {},
   "source": [
    "Despite the relationship being significant, we see the strength of the relationship is weak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004a4bf",
   "metadata": {},
   "source": [
    "If our data violates the assumptions set forth by the Point biserial correlation, then we would have to look into other methods that are less restrictive. One such method is **logistic regression**. We will discuss this method next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e1ebe",
   "metadata": {},
   "source": [
    "## Statistical hypothesis tests\n",
    "\n",
    "Sticking with the trend of asssessing the relationship between two variables, the statistical test we will focus on is the **chi squared test**. This test determines if there is a statistical difference between the expected and observed frequencies for categories in a contigency table (cross table). Observed frequencies are those that exist in the data, whereas expected frequencies are calculated from the data in the table.\n",
    "\n",
    "Chi square assumptions include:\n",
    "1. Both variables are categorical.\n",
    "2. All observations are independent.\n",
    "3. Cells in the contingency table are mutually exclusive.\n",
    "4. Expected value of cells should be 5 or greater in at least 80% of cells.\n",
    "\n",
    "Let's first create a cross table of the two variables we are interested in testing: smoking_status and heart_disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3de154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = pd.crosstab(df['smoking_status'], df['heart_disease'])\n",
    "data = pd.crosstab(df['heart_disease'],df['smoking_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89dffaf",
   "metadata": {},
   "source": [
    "Next, let's use the `chi2_contingency` function in `scipy.stats` to calulate the expected frequnencies, chi squared test statistic, and the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33304b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq, p, dof, expected = stats.chi2_contingency(data)\n",
    "print(f\"Chi square test statisic = {chisq} and p-value = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa4481",
   "metadata": {},
   "source": [
    "Using these results, we can reject the null hypothesis and state that there is a difference between the categories.\n",
    "\n",
    "We can further understand this relationship by assessing the effect size (strength of relationship) between heart_disease and smoking_status. To do so we will use **Cramer's V**, which is calculated by taking the square root of the chi squared statistic divided by the sample size and the minimum dimension minus (fewest number of categories between the two variables). The result falls between 0 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa111624",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.sqrt((chisq/len(df)) / np.min([len(data.index),len(data.columns)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cramer's V = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9fcb3",
   "metadata": {},
   "source": [
    "Despite the difference in frequencies from the chi squared test, the Cramer's V denotes little to no relationship between the two variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
